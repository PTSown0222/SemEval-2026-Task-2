{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# Handle safetensors import\n",
    "try:\n",
    "    from safetensors.torch import load_file as safe_load_file\n",
    "except ImportError:\n",
    "    print(\"Warning: 'safetensors' library not found. Will attempt to use torch.load if .bin file exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e955608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory: /Users/theson/Documents/SEMEVAL2026FT/Subtask2a\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1.CONFIGURATION \n",
    "# ============================================================\n",
    "class Config: \n",
    "    try:\n",
    "        BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        BASE_DIR = os.getcwd()\n",
    "        if \"Subtask2a\" not in BASE_DIR and os.path.exists(os.path.join(BASE_DIR, \"Subtask2a\")):\n",
    "            BASE_DIR = os.path.join(BASE_DIR, \"Subtask2a\")\n",
    "\n",
    "    print(f\"Working Directory: {BASE_DIR}\")\n",
    "    \n",
    "    WEIGHTS_DIR = os.path.join(BASE_DIR, \"weights\")\n",
    "    DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "    \n",
    "    # File Output\n",
    "    OUTPUT_FILE = os.path.join(BASE_DIR, \"submission.csv\")\n",
    "\n",
    "    # --- MODEL PARAMETERS ---\n",
    "    base_model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    window_size = 8\n",
    "    max_seq_length = 512\n",
    "    batch_size = 32\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # MoE Config\n",
    "    num_experts = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9bd6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. MODEL DEFINITION \n",
    "# ============================================================\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, 1)\n",
    "        )\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        w = self.attention(last_hidden_state) \n",
    "        w = w.squeeze(-1).masked_fill(attention_mask == 0, -1e4)\n",
    "        weights = torch.softmax(w, dim=1).unsqueeze(-1)\n",
    "        context_vector = torch.sum(weights * last_hidden_state, dim=1)\n",
    "        return context_vector\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class SparseMoELayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts=4, top_k=2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "        self.experts = nn.ModuleList([Expert(input_dim, hidden_dim, output_dim) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_logits = self.gate(x)\n",
    "        gate_probs = F.softmax(gate_logits, dim=-1)\n",
    "        topk_weights, topk_indices = torch.topk(gate_probs, self.top_k, dim=-1)\n",
    "        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)\n",
    "        all_expert_outputs = torch.stack([exp(x) for exp in self.experts], dim=1)\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        final_output = torch.zeros(batch_size, 1, device=x.device)\n",
    "        for k in range(self.top_k):\n",
    "            idx = topk_indices[:, k].view(-1, 1, 1).expand(-1, 1, all_expert_outputs.size(-1))\n",
    "            val = all_expert_outputs.gather(1, idx).squeeze(1)\n",
    "            final_output += topk_weights[:, k].unsqueeze(1) * val\n",
    "        return final_output\n",
    "\n",
    "class Subtask2aModel(nn.Module):\n",
    "    def __init__(self, model_name, num_experts=4, top_k=2):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        dim = self.config.hidden_size\n",
    "        self.pooler = AttentionPooling(dim)\n",
    "        self.valence_moe = SparseMoELayer(dim + 2, 256, 1, num_experts, top_k)\n",
    "        self.arousal_moe = SparseMoELayer(dim + 2, 256, 1, num_experts, top_k)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerical_features):\n",
    "        outputs = self.backbone(input_ids, attention_mask)\n",
    "        text_emb = self.pooler(outputs.last_hidden_state, attention_mask)\n",
    "        combined = torch.cat((text_emb, numerical_features), dim=1)\n",
    "        return self.valence_moe(combined), self.arousal_moe(combined)\n",
    "\n",
    "# ============================================================\n",
    "# 3. DATA PROCESSING\n",
    "# ============================================================\n",
    "def fix_spacing(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+'\\s+\", \"'\", text)\n",
    "    text = re.sub(r\"\\s+\\.\", \".\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def process_dataframe_for_inference(df, is_train=False):\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.sort_values(by=['user_id', 'timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    df['text'] = df['text'].apply(fix_spacing)\n",
    "    processed_data = []\n",
    "    \n",
    "    has_forecasting_marker = 'is_forecasting_user' in df.columns\n",
    "    \n",
    "    for uid, group in df.groupby('user_id'):\n",
    "        texts = group['text'].values\n",
    "        curr_v = group['valence'].values\n",
    "        curr_a = group['arousal'].values\n",
    "        \n",
    "        if is_train:\n",
    "            for i in range(len(texts)):\n",
    "                processed_data.append({'numerical_features': [curr_v[i], curr_a[i]]})\n",
    "        else:\n",
    "            target_indices = []\n",
    "            if has_forecasting_marker:\n",
    "                is_true = group['is_forecasting_user'].values\n",
    "                if 'state_change_valence' in group.columns:\n",
    "                    is_nan = np.isnan(group['state_change_valence'].values)\n",
    "                    target_indices = np.where(is_true & is_nan)[0].tolist()\n",
    "                    if not target_indices: \n",
    "                         target_indices = np.where(is_true)[0].tolist()\n",
    "                else:\n",
    "                    true_indices = np.where(is_true)[0]\n",
    "                    if len(true_indices) > 0:\n",
    "                        target_indices = [true_indices[-1]]\n",
    "            else:\n",
    "                target_indices = [len(texts) - 1]\n",
    "\n",
    "            for idx in target_indices:\n",
    "                window_texts = []\n",
    "                for k in range(Config.window_size - 1, -1, -1):\n",
    "                    i = idx - k\n",
    "                    if i >= 0: window_texts.append(str(texts[i]))\n",
    "                full_input = \" </s> \".join(window_texts)\n",
    "                \n",
    "                processed_data.append({\n",
    "                    'user_id': uid, \n",
    "                    'input_text': full_input,\n",
    "                    'numerical_features': [curr_v[idx], curr_a[idx]]\n",
    "                })\n",
    "            \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df['input_text'].values\n",
    "        self.nums = np.array(df['numerical_features'].tolist(), dtype=np.float32)\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=Config.max_seq_length, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\" : enc['input_ids'].flatten(),\n",
    "            'attention_mask': enc['attention_mask'].flatten(),\n",
    "            'numerical_features': torch.tensor(self.nums[idx], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935972c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. MAIN INFERENCE (AUTO-DETECT NESTED FOLDER)\n",
    "# ============================================================\n",
    "def predict():\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING INFERENCE SUBTASK 2A\")\n",
    "    print(f\"Working Directory: {Config.BASE_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    actual_weights_path = Config.WEIGHTS_DIR\n",
    "    if os.path.exists(Config.WEIGHTS_DIR):\n",
    "        files_in_weights = os.listdir(Config.WEIGHTS_DIR)\n",
    "        has_model_file = any(f.endswith(\".bin\") or f.endswith(\".safetensors\") for f in files_in_weights)\n",
    "        \n",
    "        if not has_model_file:\n",
    "            subfolders = [f for f in files_in_weights if os.path.isdir(os.path.join(Config.WEIGHTS_DIR, f))]\n",
    "            if subfolders:\n",
    "                print(f\" Detected nested folder. Going into: {subfolders[0]}\")\n",
    "                actual_weights_path = os.path.join(Config.WEIGHTS_DIR, subfolders[0])\n",
    "            else:\n",
    "                print(\" ERROR: Weights folder exists but contains no model files or subfolders!\")\n",
    "                return\n",
    "\n",
    "    print(f\"Target Weights Path: {actual_weights_path}\")\n",
    "\n",
    "    # --- [STEP 1] LOAD DATA & FIT SCALER ---\n",
    "    print(\">>> [1/5] Looking for Data Files...\")\n",
    "    if not os.path.exists(Config.DATA_DIR):\n",
    "        print(f\" ERROR: 'data' folder missing at {Config.DATA_DIR}\"); return\n",
    "\n",
    "    all_files = os.listdir(Config.DATA_DIR)\n",
    "    train_files = [f for f in all_files if \"train\" in f.lower()]\n",
    "    test_files = [f for f in all_files if \"forecasting\" in f.lower() or \"test\" in f.lower()]\n",
    "    test_files = [f for f in test_files if f not in train_files]\n",
    "\n",
    "    if not train_files:\n",
    "        print(\"ERROR: No training CSV found in 'data/' folder! (Required to fit Scaler)\")\n",
    "        return\n",
    "    if not test_files:\n",
    "        print(\"ERROR: No test/forecasting CSV found in 'data/' folder!\"); return\n",
    "\n",
    "    train_path = os.path.join(Config.DATA_DIR, train_files[0])\n",
    "    test_path = os.path.join(Config.DATA_DIR, test_files[0])\n",
    "    \n",
    "    print(f\"Found Train File: {train_files[0]}\")\n",
    "    print(f\"Found Test File:  {test_files[0]}\")\n",
    "\n",
    "    print(\">>> [2/5] Fitting Scaler...\")\n",
    "    df_train = pd.read_csv(train_path)\n",
    "    train_proc = process_dataframe_for_inference(df_train, is_train=True)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.array(train_proc['numerical_features'].tolist()))\n",
    "    print(\"âœ… Scaler fitted successfully!\")\n",
    "\n",
    "    print(\">>> [3/5] Processing Test Data...\")\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    test_proc = process_dataframe_for_inference(df_test, is_train=False)\n",
    "    test_nums_scaled = scaler.transform(np.array(test_proc['numerical_features'].tolist()))\n",
    "    test_proc['numerical_features'] = test_nums_scaled.tolist()\n",
    "\n",
    "    # --- [STEP 2] LOAD MODEL ---\n",
    "    print(\">>> [4/5] Loading Model...\")\n",
    "    \n",
    "    tokenizer = None\n",
    "    try:\n",
    "        print(f\"   Attempting to load tokenizer from: {actual_weights_path}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(actual_weights_path, local_files_only=True, use_fast=False)\n",
    "        print(\" Loaded tokenizer from local weights (Slow mode).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Local tokenizer failed: {e}\")\n",
    "        print(\"Downloading base tokenizer from HuggingFace...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.base_model_name, use_fast=False)\n",
    "\n",
    "    model = Subtask2aModel(Config.base_model_name, num_experts=Config.num_experts, top_k=Config.top_k)\n",
    "    \n",
    "    w_files = [f for f in os.listdir(actual_weights_path) if f.endswith('.safetensors') or f.endswith('.bin')]\n",
    "    if not w_files:\n",
    "        print(f\"ERROR: No weights found in {actual_weights_path}\"); return\n",
    "\n",
    "    w_path = os.path.join(actual_weights_path, w_files[0])\n",
    "    print(f\"Loading weights from: {w_files[0]}\")\n",
    "\n",
    "    if w_path.endswith(\".safetensors\"):\n",
    "        state_dict = safe_load_file(w_path)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "    else:\n",
    "        state_dict = torch.load(w_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    model.to(Config.device)\n",
    "    model.eval()\n",
    "\n",
    "    # --- [STEP 3] PREDICT ---\n",
    "    print(\">>> [5/5] Running Inference...\")\n",
    "    test_ds = InferenceDataset(test_proc, tokenizer)\n",
    "    test_loader = DataLoader(test_ds, batch_size=Config.batch_size, shuffle=False)\n",
    "\n",
    "    val_preds, aro_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            input_ids = batch['input_ids'].to(Config.device)\n",
    "            attention_mask = batch['attention_mask'].to(Config.device)\n",
    "            numerical_features = batch['numerical_features'].to(Config.device)\n",
    "\n",
    "            p_val, p_aro = model(input_ids, attention_mask, numerical_features)\n",
    "            val_preds.extend(p_val.cpu().numpy().flatten())\n",
    "            aro_preds.extend(p_aro.cpu().numpy().flatten())\n",
    "\n",
    "    # --- [STEP 4] SAVE ---\n",
    "    submission = pd.DataFrame({\n",
    "        'user_id': test_proc['user_id'],\n",
    "        'pred_state_change_valence': val_preds,\n",
    "        'pred_state_change_arousal': aro_preds\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(Config.OUTPUT_FILE, index=False)\n",
    "    print(f\"DONE! Submission saved to: {Config.OUTPUT_FILE}\")\n",
    "    print(submission.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semeval_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
