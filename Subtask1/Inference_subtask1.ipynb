{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df1c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from safetensors.torch import load_file as safe_load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49dcaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 1.CONFIGURATION\n",
    "# ====================================================\n",
    "class Config:\n",
    "    try:\n",
    "        BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        BASE_DIR = os.getcwd()\n",
    "        if \"Subtask1\" not in BASE_DIR and os.path.exists(os.path.join(BASE_DIR, \"Subtask1\")):\n",
    "            BASE_DIR = os.path.join(BASE_DIR, \"Subtask1\")\n",
    "\n",
    "    print(f\"Working Directory: {BASE_DIR}\")\n",
    "    \n",
    "    WEIGHTS_DIR = os.path.join(BASE_DIR, \"weights\")\n",
    "    DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "    OUTPUT_FILE = os.path.join(BASE_DIR, \"submission_subtask1.csv\")\n",
    "\n",
    "    base_model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    max_seq_length = 512\n",
    "    batch_size = 32\n",
    "    window_size = 4\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    num_experts = 4\n",
    "    top_k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dceb74f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. MODEL ARCHITECTURE (SOFT MOE + MEAN POOLING)\n",
    "# ============================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "class SoftMoEHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_experts=4, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(hidden_size, num_experts)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(hidden_size, output_dim)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_logits = self.gate(x)\n",
    "        gate_weights = torch.softmax(gate_logits, dim=1)\n",
    "        expert_outputs = torch.stack([exp(x) for exp in self.experts], dim=1)\n",
    "        # Weighted Sum\n",
    "        output = torch.sum(gate_weights.unsqueeze(-1) * expert_outputs, dim=1)\n",
    "        return output\n",
    "\n",
    "class Subtask1Model(nn.Module):\n",
    "    def __init__(self, model_name, num_experts=4):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.config.hidden_size\n",
    "\n",
    "        self.pooler = MeanPooling()\n",
    "        self.valence_moe = SoftMoEHead(hidden_size, num_experts=num_experts, output_dim=1)\n",
    "        self.arousal_moe = SoftMoEHead(hidden_size, num_experts=num_experts, output_dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        feature_vector = self.pooler(outputs.last_hidden_state, attention_mask)\n",
    "\n",
    "        val_pred = self.valence_moe(feature_vector)\n",
    "        aro_pred = self.arousal_moe(feature_vector)\n",
    "\n",
    "        return torch.cat((val_pred, aro_pred), dim=1)\n",
    "\n",
    "# ============================================================\n",
    "# 3. DATA PROCESSING\n",
    "# ============================================================\n",
    "def fix_spacing(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+'\\s+\", \"'\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def prepare_test_data(df, window_size):\n",
    "    print(f\"   -> Processing Sliding Window (Size={window_size})...\")\n",
    "    \n",
    "    # Auto-fix column names\n",
    "    possible_names = ['tweet', 'content', 'post', 'sentence', 'message']\n",
    "    for col in possible_names:\n",
    "        if col in df.columns and 'text' not in df.columns:\n",
    "            print(f\"Renaming column '{col}' -> 'text'\")\n",
    "            df = df.rename(columns={col: 'text'})\n",
    "            \n",
    "    df['text'] = df['text'].apply(fix_spacing)\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.sort_values(by=['user_id', 'timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    new_data = []\n",
    "    # Xác định cột ID (ưu tiên text_id, nếu không có thì lấy index hoặc id)\n",
    "    id_col = 'text_id' if 'text_id' in df.columns else ('id' if 'id' in df.columns else None)\n",
    "    \n",
    "    for uid, group in df.groupby('user_id'):\n",
    "        texts = group['text'].values\n",
    "        ids = group[id_col].values if id_col else range(len(texts))\n",
    "        \n",
    "        for i in range(len(texts)):\n",
    "            context_list = []\n",
    "            # Sliding Window Logic\n",
    "            for k in range(1, window_size):\n",
    "                prev_idx = i - k\n",
    "                if prev_idx >= 0:\n",
    "                    context_list.insert(0, str(texts[prev_idx]))\n",
    "            \n",
    "            current_text = str(texts[i])\n",
    "            if len(context_list) > 0:\n",
    "                context_str = ' </s> '.join(context_list)\n",
    "                full_input = f\"{context_str} </s> {current_text}\"\n",
    "            else:\n",
    "                full_input = current_text\n",
    "            \n",
    "            new_data.append({\n",
    "                'user_id': uid,\n",
    "                'text_id': ids[i],\n",
    "                'input_text': full_input\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(new_data)\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.texts = df['input_text'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].flatten(),\n",
    "            'attention_mask': enc['attention_mask'].flatten()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558520b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. MAIN INFERENCE\n",
    "# ============================================================\n",
    "def predict():\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING INFERENCE SUBTASK 1 (Soft MoE)\")\n",
    "    print(f\"Working Directory: {Config.BASE_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    actual_weights_path = Config.WEIGHTS_DIR\n",
    "    if os.path.exists(Config.WEIGHTS_DIR):\n",
    "        files = os.listdir(Config.WEIGHTS_DIR)\n",
    "        has_model = any(f.endswith(\".bin\") or f.endswith(\".safetensors\") for f in files)\n",
    "        if not has_model:\n",
    "            subfolders = [f for f in files if os.path.isdir(os.path.join(Config.WEIGHTS_DIR, f))]\n",
    "            if subfolders:\n",
    "                print(f\"Detected nested folder. Going into: {subfolders[0]}\")\n",
    "                actual_weights_path = os.path.join(Config.WEIGHTS_DIR, subfolders[0])\n",
    "            else:\n",
    "                print(\"ERROR: Weights folder empty!\"); return\n",
    "    print(f\"Target Weights Path: {actual_weights_path}\")\n",
    "\n",
    "    # --- [STEP 1] LOAD DATA (RECURSIVE SEARCH) ---\n",
    "    print(\">>> [1/4] Looking for Data Files...\")\n",
    "    if not os.path.exists(Config.DATA_DIR): print(f\"ERROR: No data folder!\"); return\n",
    "\n",
    "    test_file_path = None\n",
    "    \n",
    "    for root, dirs, files in os.walk(Config.DATA_DIR):\n",
    "        candidates = [f for f in files if (\"test\" in f.lower() or \"subtask1\" in f.lower()) \n",
    "                      and \"train\" not in f.lower() and \"val\" not in f.lower() and f.endswith(\".csv\")]\n",
    "        \n",
    "        if candidates:\n",
    "            test_file_path = os.path.join(root, candidates[0])\n",
    "            print(f\" Found Test File: {test_file_path}\")\n",
    "            break\n",
    "    \n",
    "    if not test_file_path:\n",
    "        return\n",
    "\n",
    "    # --- [STEP 2] LOAD TOKENIZER & MODEL ---\n",
    "    print(\">>> [2/4] Loading Tokenizer & Model...\")\n",
    "    tokenizer = None\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(actual_weights_path, local_files_only=True, use_fast=False)\n",
    "        print(\"Loaded tokenizer (Local/Slow).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Local tokenizer failed: {e}. Downloading base...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.base_model_name, use_fast=False)\n",
    "\n",
    "    try:\n",
    "        model = Subtask1Model(Config.base_model_name, num_experts=Config.num_experts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error init model: {e}\"); return\n",
    "\n",
    "    w_files = [f for f in os.listdir(actual_weights_path) if f.endswith('.safetensors') or f.endswith('.bin')]\n",
    "    if not w_files: print(\"No model file found!\"); return\n",
    "    w_path = os.path.join(actual_weights_path, w_files[0])\n",
    "    print(f\"   Loading weights from: {w_files[0]}\")\n",
    "    \n",
    "    if w_path.endswith(\".safetensors\"):\n",
    "        model.load_state_dict(safe_load_file(w_path), strict=False)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(w_path, map_location=\"cpu\"), strict=False)\n",
    "\n",
    "    model.to(Config.device).eval()\n",
    "\n",
    "    # --- [STEP 3] PROCESS DATA & PREDICT ---\n",
    "    print(\">>> [3/4] Processing & Predicting...\")\n",
    "    df_raw = pd.read_csv(test_file_path)\n",
    "    df_proc = prepare_test_data(df_raw, Config.window_size)\n",
    "    \n",
    "    test_ds = InferenceDataset(df_proc, tokenizer, Config.max_seq_length)\n",
    "    test_loader = DataLoader(test_ds, batch_size=Config.batch_size, shuffle=False)\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            input_ids = batch['input_ids'].to(Config.device)\n",
    "            attention_mask = batch['attention_mask'].to(Config.device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            \n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    # --- [STEP 4] POST-PROCESSING & SAVE ---\n",
    "    print(\">>> [4/4] Clipping & Saving...\")\n",
    "    \n",
    "    all_preds[:, 0] = np.clip(all_preds[:, 0], -2.0, 2.0)\n",
    "    all_preds[:, 1] = np.clip(all_preds[:, 1], 0.0, 2.0)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'user_id': df_proc['user_id'],\n",
    "        'text_id': df_proc['text_id'],\n",
    "        'pred_valence': all_preds[:, 0],\n",
    "        'pred_arousal': all_preds[:, 1]\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(Config.OUTPUT_FILE, index=False)\n",
    "    print(f\"DONE! Saved to: {Config.OUTPUT_FILE}\")\n",
    "    print(submission.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semeval_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
