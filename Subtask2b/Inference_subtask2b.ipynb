{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c90af35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# Handle safetensors import\n",
    "try:\n",
    "    from safetensors.torch import load_file as safe_load_file\n",
    "except ImportError:\n",
    "    print(\"Warning: 'safetensors' library not found. Will attempt to use torch.load if .bin file exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f0e47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory: /Users/theson/Documents/SEMEVAL2026FT/Subtask2b\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# 1. SMART CONFIGURATION\n",
    "# ====================================================\n",
    "class Config:\n",
    "    try:\n",
    "        BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        BASE_DIR = os.getcwd()\n",
    "        if \"Subtask2b\" not in BASE_DIR and os.path.exists(os.path.join(BASE_DIR, \"Subtask2b\")):\n",
    "            BASE_DIR = os.path.join(BASE_DIR, \"Subtask2b\")\n",
    "\n",
    "    print(f\"Working Directory: {BASE_DIR}\")\n",
    "    \n",
    "    WEIGHTS_DIR = os.path.join(BASE_DIR, \"weights\")\n",
    "    DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "    OUTPUT_FILE = os.path.join(BASE_DIR, \"submission.csv\")\n",
    "\n",
    "    # --- MODEL PARAMETERS ---\n",
    "    base_model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    max_seq_length = 512\n",
    "    batch_size = 16\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    num_experts = 4\n",
    "    top_k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f969f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ====================================================\n",
    "def clean_text(texts):\n",
    "    return [re.sub(r\"\\s+\", \" \", str(t)).strip() for t in texts]\n",
    "\n",
    "def calculate_user_features(df):\n",
    "    # Logic: Tính trung bình valence/arousal của Group 1\n",
    "    if 'group' not in df.columns:\n",
    "        # Fallback nếu file test không có cột group (hiếm gặp)\n",
    "        print(\"   ⚠️ Warning: 'group' column missing. Using mean of all user rows.\")\n",
    "        user_feats = df.groupby('user_id')[['valence', 'arousal']].mean()\n",
    "    else:\n",
    "        g1_df = df[df['group'] == 1]\n",
    "        user_feats = g1_df.groupby('user_id')[['valence', 'arousal']].mean()\n",
    "    \n",
    "    user_feats.columns = ['mean_valence_half1', 'mean_arousal_half1']\n",
    "    \n",
    "    # Đảm bảo đủ user\n",
    "    all_users = df['user_id'].unique()\n",
    "    user_feats = user_feats.reindex(all_users).fillna(0.0)\n",
    "    return user_feats\n",
    "\n",
    "# ====================================================\n",
    "# 3. MODEL ARCHITECTURE (MEAN POOLING - KHÁC 2A)\n",
    "# ====================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(), nn.Dropout(0.3), nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class SparseMoELayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts=4, top_k=2):\n",
    "        super().__init__()\n",
    "        self.num_experts, self.top_k = num_experts, top_k\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "        self.experts = nn.ModuleList([Expert(input_dim, hidden_dim, output_dim) for _ in range(num_experts)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gate_probs = F.softmax(self.gate(x), dim=-1)\n",
    "        topk_weights, topk_indices = torch.topk(gate_probs, self.top_k, dim=-1)\n",
    "        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)\n",
    "        all_expert_outputs = torch.stack([exp(x) for exp in self.experts], dim=1) \n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        output_dim = all_expert_outputs.size(-1)\n",
    "        final_output = torch.zeros(batch_size, output_dim, device=x.device)\n",
    "        \n",
    "        for k in range(self.top_k):\n",
    "            idx_k = topk_indices[:, k]\n",
    "            weight_k = topk_weights[:, k].unsqueeze(1)\n",
    "            idx_k_expanded = idx_k.view(-1, 1, 1).expand(-1, 1, output_dim)\n",
    "            val = all_expert_outputs.gather(1, idx_k_expanded)\n",
    "            final_output += weight_k * val.squeeze(1)\n",
    "        return final_output\n",
    "\n",
    "class Subtask2bModel(nn.Module):\n",
    "    def __init__(self, model_name, num_experts=4, top_k=2):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        dim = self.config.hidden_size\n",
    "        self.pooler = MeanPooling()\n",
    "        self.valence_head = SparseMoELayer(dim + 2, 256, 1, num_experts, top_k)\n",
    "        self.arousal_head = SparseMoELayer(dim + 2, 256, 1, num_experts, top_k)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerical_features):\n",
    "        outputs = self.backbone(input_ids, attention_mask)\n",
    "        text_emb = self.pooler(outputs.last_hidden_state, attention_mask)\n",
    "        combined = torch.cat((text_emb, numerical_features), dim=1)\n",
    "        return torch.cat((self.valence_head(combined), self.arousal_head(combined)), dim=1)\n",
    "\n",
    "# ====================================================\n",
    "# 4. INFERENCE DATASET\n",
    "# ====================================================\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, user_feature_map):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = []\n",
    "        self.user_ids = []\n",
    "\n",
    "        # Tự động fix tên cột text\n",
    "        possible_names = ['tweet', 'content', 'post', 'sentence', 'message']\n",
    "        for col in possible_names:\n",
    "            if col in df.columns and 'text' not in df.columns:\n",
    "                print(f\"   ⚠️ Renaming column '{col}' -> 'text'\")\n",
    "                df = df.rename(columns={col: 'text'})\n",
    "\n",
    "        for uid, user_df in df.groupby('user_id'):\n",
    "            self.user_ids.append(uid)\n",
    "            if 'group' in user_df.columns:\n",
    "                g1 = user_df[user_df['group'] == 1].sort_values('timestamp')\n",
    "                g2 = user_df[user_df['group'] == 2].sort_values('timestamp')\n",
    "            else:\n",
    "                # Fallback nếu không chia group: lấy nửa đầu nửa sau\n",
    "                user_df = user_df.sort_values('timestamp')\n",
    "                mid = len(user_df) // 2\n",
    "                g1, g2 = user_df.iloc[:mid], user_df.iloc[mid:]\n",
    "            \n",
    "            t1_list = clean_text(g1['text'].tail(5).tolist())\n",
    "            t2_list = clean_text(g2['text'].tail(5).tolist())\n",
    "            \n",
    "            text_g1 = \" \".join(t1_list) if t1_list else \"\"\n",
    "            text_g2 = \" \".join(t2_list) if t2_list else \"\"\n",
    "            num_feats = user_feature_map.get(uid, [0.0, 0.0])\n",
    "\n",
    "            self.data.append({'text_g1': text_g1, 'text_g2': text_g2, 'num_feats': num_feats})\n",
    "\n",
    "    def __len__(self): return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        tokens_g1 = self.tokenizer.encode(item['text_g1'], add_special_tokens=False)\n",
    "        tokens_g2 = self.tokenizer.encode(item['text_g2'], add_special_tokens=False)\n",
    "        \n",
    "        half_len = (Config.max_seq_length - 3) // 2\n",
    "        if len(tokens_g1) > half_len: tokens_g1 = tokens_g1[-half_len:]\n",
    "        if len(tokens_g2) > half_len: tokens_g2 = tokens_g2[-half_len:]\n",
    "            \n",
    "        input_ids = ([self.tokenizer.cls_token_id] + tokens_g1 + \n",
    "                     [self.tokenizer.sep_token_id, self.tokenizer.sep_token_id] + \n",
    "                     tokens_g2 + [self.tokenizer.sep_token_id])\n",
    "        \n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_len = Config.max_seq_length - len(input_ids)\n",
    "        if padding_len > 0:\n",
    "            input_ids += [self.tokenizer.pad_token_id] * padding_len\n",
    "            attention_mask += [0] * padding_len\n",
    "            \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'numerical_features': torch.tensor(item['num_feats'], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f22b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 5. MAIN FUNCTION\n",
    "# ====================================================\n",
    "def predict():\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING INFERENCE SUBTASK 2B\")\n",
    "    print(f\"Working Directory: {Config.BASE_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- [STEP 0] AUTO-DETECT WEIGHTS ---\n",
    "    actual_weights_path = Config.WEIGHTS_DIR\n",
    "    if os.path.exists(Config.WEIGHTS_DIR):\n",
    "        files = os.listdir(Config.WEIGHTS_DIR)\n",
    "        has_model = any(f.endswith(\".bin\") or f.endswith(\".safetensors\") for f in files)\n",
    "        if not has_model:\n",
    "            subfolders = [f for f in files if os.path.isdir(os.path.join(Config.WEIGHTS_DIR, f))]\n",
    "            if subfolders:\n",
    "                print(f\"Detected nested folder. Going into: {subfolders[0]}\")\n",
    "                actual_weights_path = os.path.join(Config.WEIGHTS_DIR, subfolders[0])\n",
    "            else:\n",
    "                print(\"ERROR: Weights folder empty!\"); return\n",
    "    print(f\"Target Weights Path: {actual_weights_path}\")\n",
    "\n",
    "    # --- [STEP 1] LOAD DATA ---\n",
    "    print(\">>> [1/5] Looking for Data Files...\")\n",
    "    if not os.path.exists(Config.DATA_DIR): print(f\"ERROR: No data folder!\"); return\n",
    "\n",
    "    all_files = os.listdir(Config.DATA_DIR)\n",
    "    train_files = [f for f in all_files if \"train\" in f.lower()]\n",
    "    test_files = [f for f in all_files if (\"forecasting\" in f.lower() or \"test\" in f.lower()) and f not in train_files]\n",
    "\n",
    "    if not train_files: print(\" ERROR: Missing train file (for Scaler)!\"); return\n",
    "    if not test_files: print(\" ERROR: Missing test file!\"); return\n",
    "    \n",
    "    print(f\"   Train: {train_files[0]}\")\n",
    "    print(f\"   Test:  {test_files[0]}\")\n",
    "\n",
    "    # --- [STEP 2] FIT SCALER ---\n",
    "    print(\">>> [2/5] Fitting Scaler...\")\n",
    "    scaler = StandardScaler()\n",
    "    df_train = pd.read_csv(os.path.join(Config.DATA_DIR, train_files[0]))\n",
    "    train_feats = calculate_user_features(df_train)\n",
    "    scaler.fit(train_feats.fillna(0.0))\n",
    "    print(\" Scaler fitted!\")\n",
    "\n",
    "    # --- [STEP 3] PROCESS TEST DATA ---\n",
    "    print(\">>> [3/5] Processing Test Data...\")\n",
    "    df_test = pd.read_csv(os.path.join(Config.DATA_DIR, test_files[0]))\n",
    "    \n",
    "    # Lấy danh sách forecasting user\n",
    "    if 'is_forecasting_user' in df_test.columns:\n",
    "        forecasting_users = df_test[df_test['is_forecasting_user'] == True]['user_id'].unique()\n",
    "        print(f\" Found {len(forecasting_users)} forecasting users.\")\n",
    "    else:\n",
    "        forecasting_users = df_test['user_id'].unique()\n",
    "\n",
    "    test_feats = calculate_user_features(df_test)\n",
    "    scaled_feats = scaler.transform(test_feats)\n",
    "    user_feat_map = {uid: scaled_feats[i] for i, uid in enumerate(test_feats.index)}\n",
    "\n",
    "    # --- [STEP 4] LOAD MODEL ---\n",
    "    print(\">>> [4/5] Loading Model...\")\n",
    "    tokenizer = None\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(actual_weights_path, local_files_only=True, use_fast=False)\n",
    "        print(\" Loaded tokenizer (Local/Slow).\")\n",
    "    except Exception as e:\n",
    "        print(f\" Local tokenizer failed: {e}. Downloading base...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.base_model_name, use_fast=False)\n",
    "\n",
    "    model = Subtask2bModel(Config.base_model_name, num_experts=Config.num_experts, top_k=Config.top_k)\n",
    "    \n",
    "    w_files = [f for f in os.listdir(actual_weights_path) if f.endswith('.safetensors') or f.endswith('.bin')]\n",
    "    w_path = os.path.join(actual_weights_path, w_files[0])\n",
    "    \n",
    "    if w_path.endswith(\".safetensors\"):\n",
    "        model.load_state_dict(safe_load_file(w_path), strict=False)\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(w_path, map_location=\"cpu\"), strict=False)\n",
    "    \n",
    "    model.to(Config.device).eval()\n",
    "\n",
    "    # --- [STEP 5] PREDICT & SAVE ---\n",
    "    print(\">>> [5/5] Running Inference...\")\n",
    "    test_ds = InferenceDataset(df_test, tokenizer, user_feat_map)\n",
    "    test_loader = DataLoader(test_ds, batch_size=Config.batch_size, shuffle=False)\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            input_ids = batch['input_ids'].to(Config.device)\n",
    "            attention_mask = batch['attention_mask'].to(Config.device)\n",
    "            num_feats = batch['numerical_features'].to(Config.device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, num_feats)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "    # Filter & Save\n",
    "    full_submission = pd.DataFrame({\n",
    "        'user_id': test_ds.user_ids,\n",
    "        'pred_dispo_change_valence': all_preds[:, 0],\n",
    "        'pred_dispo_change_arousal': all_preds[:, 1]\n",
    "    })\n",
    "    \n",
    "    final_submission = full_submission[full_submission['user_id'].isin(forecasting_users)]\n",
    "    final_submission.to_csv(Config.OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"DONE! Saved to: {Config.OUTPUT_FILE}\")\n",
    "    print(final_submission.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semeval_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
